{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn import mixture\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from csv import reader\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vect(train, ngram=None):\n",
    "   #takes in two arrays of strings\n",
    "   #vectorizes both from the dictionary formed from the first array\n",
    "   #returns both vectorized with the dictionary created\n",
    "   \n",
    "   #turn into bag of words representation\n",
    "    if(ngram==None):\n",
    "        vectorizer = CountVectorizer()\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(ngram_range=ngram)\n",
    "        \n",
    "    \n",
    "    X = vectorizer.fit_transform(train)\n",
    "\n",
    "    nary = vectorizer.get_feature_names()\n",
    "    res = X.toarray()\n",
    "\n",
    "    return res, nary\n",
    "\n",
    "\n",
    "def preprocess(file_train, column_text, column_tag, ngram=None):\n",
    "   #takes in a file name (including path) of a csv file\n",
    "   #and the column index of the column that contains the text of the tweets (0-based)\n",
    "   #processes the tweets into bag of words representation for further processing\n",
    "   #returns a vector of the processed tweets and a dictionary\n",
    "\n",
    "   #TRAINING DATA\n",
    "   #read in the file as a list of lists\n",
    "    with open(file_train, 'r') as read_obj:\n",
    "    # pass the file object to reader() to get the reader object\n",
    "        csv_reader = reader(read_obj)\n",
    "    # Pass reader object to list() to get a list of lists\n",
    "        list_of_rows = list(csv_reader)\n",
    "    \n",
    "   #extract the tweet texts only\n",
    "   #original is a list of all the tweets in original full-text form\n",
    "    original = []\n",
    "   #my computer runs out of memory if doing full training dataset at once\n",
    "   #so only with 10k max for now\n",
    "    for i in range(1, n_total+1):\n",
    "        original.append(list_of_rows[i][column_text])\n",
    "\n",
    "    #first half as train second half as test\n",
    "    res, nary = vect(original,ngram)\n",
    "    \n",
    "    answers = []\n",
    "    for i in range(1, n_total+1):\n",
    "        answers.append(list_of_rows[i][column_tag])\n",
    "         \n",
    "    return res, nary, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on this tutorial from sklearn:\n",
    "#https://scikit-learn.org/stable/auto_examples/neighbors/plot_digits_kde_sampling.html#sphx-glr-auto-examples-neighbors-plot-digits-kde-sampling-py\n",
    "def sentence_generator(dataset, pca_divider = 4, total_samples = 10):\n",
    "    \n",
    "    data_size,feature_size = dataset.shape\n",
    "    max_pca = min(data_size, feature_size)\n",
    "    pca_components = min(feature_size//pca_divider, max_pca)\n",
    "    pca = PCA(n_components=pca_components)\n",
    "    #Find first 50 principle componenets\n",
    "    dataset_pca = pca.fit_transform(dataset)\n",
    "\n",
    "    #tune hyper-pramaters and perform density estimation\n",
    "    hyper_parameters = {'bandwidth': numpy.logspace(-1, 1, 50)}\n",
    "    grid = GridSearchCV(KernelDensity(), hyper_parameters, iid=True, cv=3)\n",
    "    grid.fit(dataset_pca)\n",
    "    density = grid.best_estimator_\n",
    "\n",
    "\n",
    "    samples = density.sample(10)\n",
    "    samples = pca.inverse_transform(samples)\n",
    "    samples = numpy.absolute(numpy.around(samples)).astype('int32')\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "for data size  100\n",
      "\n",
      "\n",
      "sentences for PCA with 1/4 of original features size\n",
      "amp be beach could danmerriweather go great home jersey me miss now on see shontelle_layne t4 the to wish would \n",
      "amp bed day get have immediately mobile mom new off paying phone says she this to want \n",
      "and baddest complained entertainment eveer experiment experimental for he is melody over properly rupturerapture say should someone the with you \n",
      "amp but chillin juuuuuuuuuuuuuuuuussssst to too \n",
      "and but enough floyd great has in need now relievers said scolding sox was \n",
      "be friendly is lol may the think to too want well \n",
      "about australia chillin cough iran jersey juuuuuuuuuuuuuuuuussssst new now sad think to very was what wompp wompppp \n",
      "120gb 37 anyonefeeling backwards bend generous guitar harddrive inch new or tv want \n",
      "amp com enough for guys http hurts is it love much one said so that this tumblr xkh1z19us \n",
      "com earl ever http jin longest night now the this tumblr twitter ugh xwp1yxhi6 \n",
      "\n",
      "\n",
      "sentences for PCA with 1/2 of original features size\n",
      "amp any boring him jin me miss please tell whats with wrong you \n",
      "amp days doing gonna jersey jin need no off sat this to twitter week work \n",
      "celebrate cough feeling fine go gonna like listen million proud semisonic some strangely to work \n",
      "15ssci and bit http is it ly meat meet the your \n",
      "days just need no now of off or sat to week work \n",
      "chillin jersey juuuuuuuuuuuuuuuuussssst miss new the \n",
      "about always be friendly iran lol may sad think too very well \n",
      "acl an any appears athlete going hate it leysh live of on or t9ar5 tear television to was when \n",
      "and backwards bend call class com earl enough from got health http in joke last much new office on said so someone something the tumblr week wrote xcn21w6o7 york you \n",
      "for friend hug is just much or the to tonight too worry \n",
      "\n",
      "\n",
      "\n",
      "for data size  545\n",
      "\n",
      "\n",
      "sentences for PCA with 1/4 of original features size\n",
      "2moz for going paddle \n",
      "be friendly lol may think too well \n",
      "day guess more no nother off ohhhh say school sickness strike well \n",
      "62 abc akqld at bit cancer dies farrah fawcett http ly news of turner via \n",
      "already her miss much so \n",
      "bed now time \n",
      "bball bf game go grrr let me mi momacita my to won \n",
      "ckbmkc com day having http luvly tinyurl \n",
      "get guna is more no off one probably since soon talkin \n",
      "battleground \n",
      "\n",
      "\n",
      "sentences for PCA with 1/2 of original features size\n",
      "am and are com fine frank gay hello hi how http lampard stranger tumblr urself xaj24dkly you your \n",
      "and chocolate craving funny how hungover last lol night omg pub the was \n",
      "333 love lt you \n",
      "advice any anyone break cope have how on to up with \n",
      "ever funniest is mom my person the \n",
      "fun give have let what when ya \n",
      "10h4ne all and bicyclist bit dressed for henderson http ky ly monday nite no ride up \n",
      "102 112 all day face haters in my thanks the to up \n",
      "allergies cut get hair hate my poll public should taking tomorrow \n",
      "away go rain \n",
      "\n",
      "\n",
      "\n",
      "for data size  990\n",
      "\n",
      "\n",
      "sentences for PCA with 1/4 of original features size\n",
      "__cheer emo kid up \n",
      "album already ambition and brothers concert fulfilling go hmv is jonas life me my new out sold the went why xxkirahxx \n",
      "he im lol love sure that thediamondcoach \n",
      "is lazzzzyyyy life my very \n",
      "forever hate you \n",
      "album already ambition and brothers concert fulfilling go hmv is jonas life me my new out sold the went why xxkirahxx \n",
      "amp day enjoy for ladykeisha post scorpio510 thanx the ur warm wishes \n",
      "hmmmm how my number she wonder \n",
      "amp atm but clubbing come conf conference etc is must off over party probably really sheffield shouldn sometime tech to warbo \n",
      "beard been cut for gonna growing happy in is it just meantime my off only over shaunamanu start the well year \n",
      "\n",
      "\n",
      "sentences for PCA with 1/2 of original features size\n",
      "byyeee ily2 jamarcusssssss \n",
      "and at bored boyfie com damn do great have http idea im iâ just missing moment no on the to too tumblr what xzp224xgj \n",
      "bye world \n",
      "commie commies crush grandfather my quot rip sad says tellman the was \n",
      "45 but chihuahua cranberryperson dear drive have http hyper im laj6 left minutes on only this together tr we \n",
      "and could cuz gonna im is it just more not smile sure they want what \n",
      "bas last my one status time update \n",
      "69ays alarming an bbc bit change climate forecasts http is it ly maps news one paints picture quot the uk \n",
      "already done get getting my psych quizzes really sleepy to tonight want \n",
      "here hoping katherineholden nervous pretty thanks though \n"
     ]
    }
   ],
   "source": [
    "file_train = \"train.csv\"\n",
    "def print_samples(samples):\n",
    "    total_samples,dimensions = samples.shape\n",
    "    \n",
    "    for sample in range(total_samples):\n",
    "        for i in range(dimensions):\n",
    "            if(samples[sample,i] != 0):\n",
    "                print(dictionary[i], end = ' ')\n",
    "        print()\n",
    "    \n",
    "\n",
    "for data_size in range(100,1000,445):\n",
    "    n_total = data_size\n",
    "    dataset,dictionary,labels = preprocess(file_train, 2, 1)\n",
    "    print(\"\\n\\n\\nfor data size \",n_total)\n",
    "    print(\"\\n\\nsentences for PCA with 1/4 of original features size\")\n",
    "    samples = sentence_generator(dataset, pca_divider=4, total_samples = 10)\n",
    "    print_samples(samples)\n",
    "    print(\"\\n\\nsentences for PCA with 1/2 of original features size\")\n",
    "    samples = sentence_generator(dataset, pca_divider=2, total_samples = 10)\n",
    "    print_samples(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here n-gram of 3 is used instead of bag-of-words and we can see dependency between words such as subject etc. are captured\n",
      "\n",
      "rain go away rain rain go \n",
      "bunch of frownies is the story life whole bunch lt that is my life whole of my life story of my that is the the story of whole bunch of \n",
      "day off school more sickness strike no more sickness nother day off off school guess ohhhh well nother say strike ohhhh sickness strike say strike ohhhh well strike say strike well nother day \n",
      "anoron reedcourty operaunite com webserver content content home sch home sch alternatã operaunite com webserver reedcourty operaunite com sch alternatã va webserver content home \n",
      "anticipating tough day tough day ahead \n",
      "must say more \n",
      "bed time now \n",
      "lot lot lot \n",
      "friends on twitter have noooooooooo friends it makes me makes me sad me sad will noooooooooo friends on on twitter it sad will someone someone follow me twitter it makes will someone follow \n",
      "all having gum at all having bother me at dentists drills etc doesn bother me drills etc now etc now it gum lift surgery having gum lift immune to dentists it just doesn just doesn bother lift surgery next littlecharva immune to me at all next week tho now it just surgery next week to dentists drills week tho eek \n"
     ]
    }
   ],
   "source": [
    "print(\"Here n-gram of 3 is used instead of bag-of-words and we can see dependency between words such as subject etc. are captured\\n\")\n",
    "n_total = 500\n",
    "dataset,dictionary,labels = preprocess(file_train, 2, 1,ngram=(3,3))\n",
    "samples = sentence_generator(dataset, pca_divider=2, total_samples = 10)\n",
    "print_samples(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
